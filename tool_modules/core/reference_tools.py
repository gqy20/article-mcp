"""
ç»Ÿä¸€å‚è€ƒæ–‡çŒ®å·¥å…· - æ ¸å¿ƒå·¥å…·3
"""

import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any

# å…¨å±€æœåŠ¡å®ä¾‹
_reference_services = None


def register_reference_tools(mcp, services, logger):
    """æ³¨å†Œå‚è€ƒæ–‡çŒ®å·¥å…·"""
    global _reference_services
    _reference_services = services

    @mcp.tool()
    def get_references(
        identifier: str,
        id_type: str = "doi",
        sources: list[str] = ["europe_pmc", "crossref"],
        max_results: int = 20,
        include_metadata: bool = True,
    ) -> dict[str, Any]:
        """è·å–å‚è€ƒæ–‡çŒ®å·¥å…·

        ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
        - è·å–æŒ‡å®šæ–‡çŒ®çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
        - æ”¯æŒå¤šç§æ–‡çŒ®æ ‡è¯†ç¬¦ç±»å‹
        - æä¾›å®Œæ•´çš„å¼•ç”¨ä¿¡æ¯

        ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹ï¼š
        1. get_references("10.1038/s41586-021-03819-2")
        2. get_references("34567890", id_type="pmid", max_results=50)
        3. get_references("arXiv:2101.00001", sources=["arxiv"])

        ğŸ”§ å‚æ•°è¯´æ˜ï¼š
        - identifier: æ–‡çŒ®æ ‡è¯†ç¬¦ (æ”¯æŒDOIã€PMIDã€PMCID)
        - id_type: æ ‡è¯†ç¬¦ç±»å‹ ["auto"(è‡ªåŠ¨è¯†åˆ«), "doi", "pmid", "pmcid"]
        - sources: æ•°æ®æºåˆ—è¡¨ (ç›®å‰ä¸»è¦æ”¯æŒ europe_pmc)
        - max_results: æœ€å¤§è¿”å›å‚è€ƒæ–‡çŒ®æ•°é‡ (å»ºè®®20-100)
        - include_metadata: æ˜¯å¦åŒ…å«è¯¦ç»†å…ƒæ•°æ®

        âœ… æ¨èç”¨æ³•ï¼š
        - è·å–å¼•ç”¨å…³ç³»ï¼šä¼ å…¥æ–‡çŒ®DOIï¼Œè·å–å…¶å‚è€ƒæ–‡çŒ®
        - å¤§é‡å‚è€ƒæ–‡çŒ®ï¼šå¢åŠ  max_results å‚æ•°
        - ç‰¹å®šæ•°æ®æºï¼šæŒ‡å®š sources å‚æ•°

        ğŸ“Š è¿”å›æ ¼å¼ï¼š
        {
            "success": true,
            "identifier": "æŸ¥è¯¢çš„æ–‡çŒ®æ ‡è¯†ç¬¦",
            "id_type": "è¯†åˆ«å‡ºçš„æ ‡è¯†ç¬¦ç±»å‹",
            "sources_used": ["æˆåŠŸæŸ¥è¯¢çš„æ•°æ®æº"],
            "references_by_source": {
                "æ•°æ®æºåç§°": [å‚è€ƒæ–‡çŒ®åˆ—è¡¨]
            },
            "merged_references": [å»é‡åˆå¹¶åçš„å‚è€ƒæ–‡çŒ®],
            "total_count": æ€»å‚è€ƒæ–‡çŒ®æ•°é‡,
            "processing_time": å¤„ç†è€—æ—¶(ç§’)
        }
        """
        try:
            if not identifier or not identifier.strip():
                return {
                    "success": False,
                    "error": "æ–‡çŒ®æ ‡è¯†ç¬¦ä¸èƒ½ä¸ºç©º",
                    "identifier": identifier,
                    "sources_used": [],
                    "references_by_source": {},
                    "merged_references": [],
                    "total_count": 0,
                }

            from src.merged_results import merge_reference_results

            start_time = time.time()
            reference_results = {}

            # å¹¶è¡Œè·å–å‚è€ƒæ–‡çŒ®
            with ThreadPoolExecutor(max_workers=3) as executor:
                future_to_source = {}

                for source in sources:
                    if source not in _reference_services:
                        continue

                    try:
                        service = _reference_services[source]
                        if source == "europe_pmc":
                            # Europe PMCéœ€è¦ä»æ–‡çŒ®è¯¦æƒ…ä¸­è·å–å‚è€ƒæ–‡çŒ®
                            detail_result = service.fetch(identifier.strip(), id_type=id_type)
                            if detail_result.get("success", False):
                                article = detail_result.get("article", {})
                                references = article.get("references", [])
                                reference_results[source] = {
                                    "success": True,
                                    "references": references,
                                    "total_count": len(references),
                                    "source": source,
                                }
                        elif source == "crossref":
                            ref_result = service.get_references(identifier.strip(), max_results)
                            reference_results[source] = ref_result
                        elif source == "openalex":
                            # OpenAlexæš‚æ—¶æ²¡æœ‰ç›´æ¥çš„å‚è€ƒæ–‡çŒ®API
                            reference_results[source] = {
                                "success": False,
                                "references": [],
                                "total_count": 0,
                                "source": source,
                                "error": "OpenAlexæš‚ä¸æ”¯æŒå‚è€ƒæ–‡çŒ®æŸ¥è¯¢",
                            }

                    except Exception as e:
                        logger.error(f"{source} è·å–å‚è€ƒæ–‡çŒ®å¼‚å¸¸: {e}")
                        reference_results[source] = {
                            "success": False,
                            "references": [],
                            "total_count": 0,
                            "source": source,
                            "error": str(e),
                        }

            # åˆå¹¶å‚è€ƒæ–‡çŒ®
            merged_result = merge_reference_results(reference_results)
            processing_time = round(time.time() - start_time, 2)

            return {
                **merged_result,
                "identifier": identifier.strip(),
                "id_type": id_type,
                "processing_time": processing_time,
                "include_metadata": include_metadata,
            }

        except Exception as e:
            logger.error(f"è·å–å‚è€ƒæ–‡çŒ®å¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": str(e),
                "identifier": identifier,
                "sources_used": [],
                "references_by_source": {},
                "merged_references": [],
                "total_count": 0,
                "processing_time": 0,
            }

    @mcp.tool()
    def batch_process_articles(
        identifiers: list[str],
        operations: list[str] = ["details", "quality"],
        parallel: bool = True,
        max_concurrent: int = 10,
    ) -> dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡çŒ®å·¥å…·

        åŠŸèƒ½è¯´æ˜ï¼š
        - æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡çŒ®æ ‡è¯†ç¬¦
        - æ”¯æŒå¤šç§æ“ä½œç±»å‹
        - å¯é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œå¤„ç†

        å‚æ•°è¯´æ˜ï¼š
        - identifiers: æ–‡çŒ®æ ‡è¯†ç¬¦åˆ—è¡¨
        - operations: æ“ä½œç±»å‹åˆ—è¡¨ ["details", "quality", "relations", "references"]
        - parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†
        - max_concurrent: æœ€å¤§å¹¶å‘æ•°

        è¿”å›æ ¼å¼ï¼š
        {
            "success": true,
            "processed_count": 5,
            "total_count": 5,
            "results": {...},
            "processing_time": 3.45
        }
        """
        try:
            if not identifiers:
                return {
                    "success": False,
                    "error": "æ–‡çŒ®æ ‡è¯†ç¬¦åˆ—è¡¨ä¸èƒ½ä¸ºç©º",
                    "processed_count": 0,
                    "total_count": 0,
                    "results": {},
                    "processing_time": 0,
                }

            from tool_modules.core.article_tools import _article_services
            from tool_modules.core.search_tools import _search_services

            start_time = time.time()
            results = {}

            if parallel:
                # å¹¶è¡Œå¤„ç†
                with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                    future_to_identifier = {}

                    for identifier in identifiers:
                        for operation in operations:
                            future = executor.submit(
                                _process_single_article,
                                identifier.strip(),
                                operation,
                                _search_services,
                                _article_services,
                                logger,
                            )
                            future_to_identifier[future] = (identifier, operation)

                    for future in as_completed(future_to_identifier):
                        identifier, operation = future_to_identifier[future]
                        try:
                            result = future.result()
                            if identifier not in results:
                                results[identifier] = {}
                            results[identifier][operation] = result
                        except Exception as e:
                            logger.error(f"å¤„ç† {identifier} çš„ {operation} æ“ä½œå¤±è´¥: {e}")
                            if identifier not in results:
                                results[identifier] = {}
                            results[identifier][operation] = {"success": False, "error": str(e)}
            else:
                # ä¸²è¡Œå¤„ç†
                for identifier in identifiers:
                    identifier_results = {}
                    for operation in operations:
                        try:
                            result = _process_single_article(
                                identifier.strip(),
                                operation,
                                _search_services,
                                _article_services,
                                logger,
                            )
                            identifier_results[operation] = result
                        except Exception as e:
                            logger.error(f"å¤„ç† {identifier} çš„ {operation} æ“ä½œå¤±è´¥: {e}")
                            identifier_results[operation] = {"success": False, "error": str(e)}
                    results[identifier] = identifier_results

            processing_time = round(time.time() - start_time, 2)
            processed_count = len(results)
            successful_count = sum(
                1 for r in results.values() if any(op.get("success", False) for op in r.values())
            )

            return {
                "success": successful_count > 0,
                "processed_count": processed_count,
                "total_count": len(identifiers),
                "successful_count": successful_count,
                "results": results,
                "processing_time": processing_time,
                "operations": operations,
                "parallel": parallel,
            }

        except Exception as e:
            from src.error_utils import format_error

            logger.error(f"æ‰¹é‡å¤„ç†æ–‡çŒ®å¼‚å¸¸: {e}")
            return format_error(
                "batch_process_articles",
                e,
                {
                    "processed_count": 0,
                    "total_count": len(identifiers) if identifiers else 0,
                    "successful_count": 0,
                    "results": {},
                    "processing_time": 0,
                },
            )

    return [get_references, batch_process_articles]


def _process_single_article(
    identifier: str, operation: str, search_services, article_services, logger
) -> dict[str, Any]:
    """å¤„ç†å•ä¸ªæ–‡çŒ®çš„æ“ä½œ"""
    try:
        if operation == "details":
            # ä½¿ç”¨article_servicesè·å–è¯¦æƒ…
            if article_services:
                sources = ["europe_pmc", "crossref", "openalex"]
                details_result = _get_article_details_internal(
                    identifier, "auto", sources, article_services, logger
                )
                return details_result
        elif operation == "quality":
            # è·å–è´¨é‡æŒ‡æ ‡
            return {
                "success": True,
                "identifier": identifier,
                "operation": operation,
                "message": "è´¨é‡è¯„ä¼°åŠŸèƒ½å¾…å®ç°",
            }
        elif operation == "relations":
            # è·å–æ–‡çŒ®å…³è”ä¿¡æ¯
            return {
                "success": True,
                "identifier": identifier,
                "operation": operation,
                "message": "å…³ç³»åˆ†æåŠŸèƒ½å¾…å®ç°",
            }
        elif operation == "references":
            # è·å–å‚è€ƒæ–‡çŒ®
            # è¿™é‡Œä¸èƒ½ç›´æ¥è°ƒç”¨get_referencesï¼Œå› ä¸ºå®ƒåœ¨å·¥å…·æ³¨å†Œåæ‰å®šä¹‰
            # è¿”å›ä¸€ä¸ªç®€åŒ–çš„ç»“æœ
            return {
                "success": True,
                "identifier": identifier,
                "operation": operation,
                "message": "å‚è€ƒæ–‡çŒ®è·å–åŠŸèƒ½å·²é›†æˆåˆ°get_referenceså·¥å…·ä¸­",
                "references": [],
            }
        else:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}",
                "identifier": identifier,
                "operation": operation,
            }

    except Exception as e:
        logger.error(f"å¤„ç†æ–‡çŒ® {identifier} çš„ {operation} æ“ä½œå¼‚å¸¸: {e}")
        return {"success": False, "error": str(e), "identifier": identifier, "operation": operation}


def _get_article_details_internal(
    identifier: str, id_type: str, sources: list[str], article_services, logger
) -> dict[str, Any]:
    """å†…éƒ¨æ–‡ç« è¯¦æƒ…è·å–å‡½æ•°"""
    if not article_services:
        return {"success": False, "error": "æœåŠ¡æœªåˆå§‹åŒ–"}

    from src.merged_results import extract_identifier_type, merge_same_doi_articles

    details_by_source = {}
    sources_found = []

    # è‡ªåŠ¨è¯†åˆ«æ ‡è¯†ç¬¦ç±»å‹
    if id_type == "auto":
        id_type = extract_identifier_type(identifier.strip())

    for source in sources:
        if source not in article_services:
            continue

        try:
            service = article_services[source]
            if source == "europe_pmc":
                result = service.fetch(identifier.strip(), id_type=id_type)
            elif source == "crossref":
                if id_type == "doi":
                    result = service.get_work_by_doi(identifier.strip())
                else:
                    continue
            elif source == "openalex":
                if id_type == "doi":
                    result = service.get_work_by_doi(identifier.strip())
                else:
                    continue
            else:
                continue

            if result.get("success", False) and result.get("article"):
                details_by_source[source] = result["article"]
                sources_found.append(source)

        except Exception as e:
            logger.error(f"{source} è·å–è¯¦æƒ…å¼‚å¸¸: {e}")
            continue

    merged_detail = None
    if details_by_source:
        articles = [details_by_source[source] for source in sources_found]
        merged_detail = merge_same_doi_articles(articles)

    return {
        "success": len(details_by_source) > 0,
        "identifier": identifier.strip(),
        "id_type": id_type,
        "sources_found": sources_found,
        "details_by_source": details_by_source,
        "merged_detail": merged_detail,
    }
