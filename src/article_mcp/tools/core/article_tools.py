"""
ç»Ÿä¸€æ–‡çŒ®è¯¦æƒ…å·¥å…· - æ ¸å¿ƒå·¥å…·2
"""

import time
from typing import Any

from fastmcp import FastMCP

# å…¨å±€æœåŠ¡å®ä¾‹
_article_services = None


def register_article_tools(mcp: FastMCP, services: dict[str, Any], logger: Any) -> None:
    """æ³¨å†Œæ–‡çŒ®è¯¦æƒ…å·¥å…·"""
    global _article_services
    _article_services = services

    @mcp.tool()
    def get_article_details(
        identifier: str,
        id_type: str = "auto",
        sources: list[str] = ["europe_pmc", "crossref"],
        include_quality_metrics: bool = False,
    ) -> dict[str, Any]:
        """è·å–æ–‡çŒ®è¯¦æƒ…å·¥å…·

        ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
        - è·å–å•ç¯‡æ–‡çŒ®çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯
        - è‡ªåŠ¨è¯†åˆ«æ ‡è¯†ç¬¦ç±»å‹
        - åˆå¹¶å¤šæºæ•°æ®ï¼Œä¿¡æ¯æ›´å®Œæ•´

        ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹ï¼š
        1. get_article_details("10.1038/s41586-021-03819-2")
        2. get_article_details("34567890", id_type="pmid")
        3. get_article_details("arXiv:2101.00001", sources=["arxiv"])
        4. get_article_details("10.1234/example", include_quality_metrics=True)

        ğŸ”§ å‚æ•°è¯´æ˜ï¼š
        - identifier: æ–‡çŒ®æ ‡è¯†ç¬¦ (æ”¯æŒDOIã€PMIDã€PMCIDã€arXiv ID)
        - id_type: æ ‡è¯†ç¬¦ç±»å‹ï¼Œå¯é€‰ ["auto"(è‡ªåŠ¨è¯†åˆ«), "doi", "pmid", "pmcid", "arxiv_id"]
        - sources: æ•°æ®æºåˆ—è¡¨ï¼Œæ¨è ["europe_pmc", "crossref", "openalex", "arxiv"]
        - include_quality_metrics: æ˜¯å¦åŒ…å«æœŸåˆŠè´¨é‡æŒ‡æ ‡ (éœ€è¦EasyScholarå¯†é’¥)

        âœ… æ¨èç”¨æ³•ï¼š
        - å·²çŸ¥DOIï¼šç›´æ¥ä¼ å…¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
        - å·²çŸ¥PMIDï¼šæŒ‡å®š id_type="pmid"
        - éœ€è¦è´¨é‡è¯„ä¼°ï¼šè®¾ç½® include_quality_metrics=True
        - æŸ¥è¯¢arXivè®ºæ–‡ï¼šæŒ‡å®š arxiv æ•°æ®æº

        ğŸ“Š è¿”å›æ ¼å¼ï¼š
        {
            "success": true,
            "identifier": "ä¼ å…¥çš„æ ‡è¯†ç¬¦",
            "id_type": "è¯†åˆ«å‡ºçš„æ ‡è¯†ç¬¦ç±»å‹",
            "sources_found": ["æˆåŠŸè·å–çš„æ•°æ®æº"],
            "details_by_source": {
                "æ•°æ®æºåç§°": {åŸå§‹æ•°æ®}
            },
            "merged_detail": {åˆå¹¶åçš„å®Œæ•´æ•°æ®},
            "quality_metrics": {æœŸåˆŠè´¨é‡æŒ‡æ ‡(å¦‚æœè¯·æ±‚)},
            "processing_time": å¤„ç†è€—æ—¶(ç§’)
        }
        """
        try:
            if not identifier or not identifier.strip():
                return {"success": False, "error": "æ–‡çŒ®æ ‡è¯†ç¬¦ä¸èƒ½ä¸ºç©º", "identifier": identifier}

            from ..services.merged_results import extract_identifier_type, merge_same_doi_articles

            start_time = time.time()
            details_by_source = {}
            sources_found = []

            # è‡ªåŠ¨è¯†åˆ«æ ‡è¯†ç¬¦ç±»å‹
            if id_type == "auto":
                id_type = extract_identifier_type(identifier.strip())

            # ä»æ¯ä¸ªæ•°æ®æºè·å–è¯¦æƒ…
            for source in sources:
                if source not in _article_services:
                    continue

                try:
                    service = _article_services[source]
                    if source == "europe_pmc":
                        result = service.fetch(identifier.strip(), id_type=id_type)
                    elif source == "crossref":
                        if id_type == "doi":
                            result = service.get_work_by_doi(identifier.strip())
                        else:
                            continue
                    elif source == "openalex":
                        if id_type == "doi":
                            result = service.get_work_by_doi(identifier.strip())
                        else:
                            continue
                    elif source == "arxiv":
                        if id_type == "arxiv_id":
                            result = service.fetch(identifier.strip(), id_type=id_type)
                        else:
                            continue
                    else:
                        continue

                    if result.get("success", False) and result.get("article"):
                        details_by_source[source] = result["article"]
                        sources_found.append(source)
                        logger.info(f"{source} è·å–è¯¦æƒ…æˆåŠŸ")
                    else:
                        logger.debug(f"{source} æœªæ‰¾åˆ°æ–‡çŒ®è¯¦æƒ…")

                except Exception as e:
                    logger.error(f"{source} è·å–è¯¦æƒ…å¼‚å¸¸: {e}")
                    continue

            # åˆå¹¶è¯¦æƒ…
            merged_detail = None
            if details_by_source:
                articles = [details_by_source[source] for source in sources_found]
                merged_detail = merge_same_doi_articles(articles)

            # è·å–è´¨é‡æŒ‡æ ‡
            quality_metrics = None
            if include_quality_metrics and merged_detail:
                journal_name = merged_detail.get("journal", "")
                if journal_name:
                    try:
                        from ..services.mcp_config import get_easyscholar_key

                        secret_key = get_easyscholar_key(None, logger)
                        pubmed_service = _article_services.get("pubmed")
                        if pubmed_service:
                            quality_metrics = pubmed_service.get_journal_quality(
                                journal_name, secret_key
                            )
                    except Exception as e:
                        logger.warning(f"è·å–æœŸåˆŠè´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")

            processing_time = round(time.time() - start_time, 2)

            return {
                "success": len(details_by_source) > 0,
                "identifier": identifier.strip(),
                "id_type": id_type,
                "sources_found": sources_found,
                "details_by_source": details_by_source,
                "merged_detail": merged_detail,
                "quality_metrics": quality_metrics,
                "processing_time": processing_time,
            }

        except Exception as e:
            logger.error(f"è·å–æ–‡çŒ®è¯¦æƒ…å¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": str(e),
                "identifier": identifier,
                "sources_found": [],
                "details_by_source": {},
                "merged_detail": None,
                "quality_metrics": None,
                "processing_time": 0,
            }

    return [get_article_details]
